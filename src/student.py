import torch
from torch.utils.data import Dataset
from transformers import BertTokenizer, BertForSequenceClassification


from .graph import KnowledgeGraph


class StudentModel(torch.nn.Module):
  """
  A lightweight classification model (the 'Student') that learns to diagnose
  the root cause of a mathematical error.

  This class wraps a pre-trained BERT model from Hugging Face and adds a
  classification head to predict the correct error concept ID.
  """

  def __init__(self, num_labels: int, model_name: str):
    """
    Initializes the StudentModel.

    Args:
        num_labels: The number of unique "failure concepts" we need to predict.
        model_name: The name of the pre-trained model to use from Hugging Face.
    """
    super(StudentModel, self).__init__()
    self.bert = BertForSequenceClassification.from_pretrained(
      model_name, num_labels=num_labels
    )

  def forward(self, input_ids, attention_mask, labels=None):
    """
    Performs a forward pass through the model.
    """
    outputs = self.bert(
      input_ids=input_ids, attention_mask=attention_mask, labels=labels
    )
    return outputs


class ErrorDiagnosisDataset(Dataset):
  """
  A PyTorch Dataset class to handle the synthetic error data.

  This class is responsible for taking the raw data generated by the
  DataGenerator and tokenizing it for consumption by the StudentModel.
  """

  def __init__(
    self,
    dataset: list[dict],
    tokenizer: BertTokenizer,
    label_map: dict,
    knowledge_graph: KnowledgeGraph,
    max_length: int = 512,  # Increased max_length for more context
  ):
    """
    Initializes the dataset.

    Args:
        dataset: The list of data examples from DataGenerator.
        tokenizer: The BERT tokenizer instance.
        label_map: A dictionary mapping concept_id strings to integer labels.
        knowledge_graph: The full knowledge graph instance.
        max_length: The maximum token length for the model input.
    """
    self.dataset = dataset
    self.tokenizer = tokenizer
    self.label_map = label_map
    self.knowledge_graph = knowledge_graph
    self.max_length = max_length

  def __len__(self):
    """Returns the number of items in the dataset."""
    return len(self.dataset)

  def __getitem__(self, idx):
    """
    Gets a single item from the dataset and prepares it for training.
    """
    item = self.dataset[idx]

    problem = item["problem_example"]
    solution = item["incorrect_solution"]
    target_concept_id = item["target_concept_id"]

    # Get the names of all prerequisite descendants for the target concept
    descendants = self.knowledge_graph.get_all_descendants(target_concept_id)
    descendant_names = ", ".join([d.name for d, _ in descendants])
    prereq_context = f"Relevant Concepts: {descendant_names}"

    # Combine all information into a single input string
    text = (
      f"Problem: {problem} [SEP] {prereq_context} [SEP] Solution: {solution}"
    )

    # The label is the specific prerequisite concept that was the root cause.
    label_id = item["failure_concept_id"]
    label = self.label_map[label_id]

    # Tokenize the text
    encoding = self.tokenizer.encode_plus(
      text,
      add_special_tokens=True,
      max_length=self.max_length,
      padding="max_length",
      truncation=True,
      return_attention_mask=True,
      return_tensors="pt",
    )

    return {
      "input_ids": encoding["input_ids"].squeeze(),  # type: ignore
      "attention_mask": encoding["attention_mask"].squeeze(),  # type: ignore
      "labels": torch.tensor(label, dtype=torch.long),
    }
